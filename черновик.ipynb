{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2152c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 08:24:44,943 : INFO : process_data : Начали скачивать данные\n",
      "2026-02-17 08:24:46,552 : INFO : process_data : Успешно скачали данные!\n",
      "2026-02-17 08:24:46,552 : INFO : process_data : Делаем предобработку данных\n",
      "2026-02-17 08:24:46,569 : INFO : process_data :     Используемые фичи: ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country']\n",
      "2026-02-17 08:24:46,709 : INFO : process_data :     Размер тренировочного датасета: 18233\n",
      "2026-02-17 08:24:46,710 : INFO : process_data :     Размер тестового датасета: 9769\n",
      "2026-02-17 08:24:46,710 : INFO : process_data : Начали сохранять датасеты\n",
      "2026-02-17 08:24:48,586 : INFO : process_data : Успешно сохранили датасеты!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from constants import DATASET_NAME, DATASET_PATH_PATTERN, TEST_SIZE, RANDOM_STATE\n",
    "from utils import get_logger, load_params\n",
    "import mlflow\n",
    "\n",
    "PREPROCESSORS = {preprocessor.__name__: preprocessor for preprocessor in [OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler]}\n",
    "STAGE_NAME = 'process_data'\n",
    "\n",
    "def process_data():\n",
    "    logger = get_logger(logger_name=STAGE_NAME)\n",
    "    params = load_params(stage_name=STAGE_NAME)\n",
    "\n",
    "    logger.info('Начали скачивать данные')\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    logger.info('Успешно скачали данные!')\n",
    "\n",
    "    logger.info('Делаем предобработку данных')\n",
    "    df = dataset['train'].to_pandas()\n",
    "    target_column = 'income'\n",
    "    columns = df.columns if params['features'] == 'all' else params['features']\n",
    "    drop_cols = set(params['cols_to_drop'])\n",
    "    columns = [col for col in columns if col not in drop_cols and col != target_column]\n",
    "\n",
    "    X, y = df[columns], df[target_column]\n",
    "    logger.info(f'    Используемые фичи: {columns}')\n",
    "\n",
    "    all_cat_features = params['cats']\n",
    "    cat_features = list(set(columns) & set(all_cat_features))\n",
    "    num_features = list(set(columns) - set(all_cat_features))\n",
    "\n",
    "    y: pd.Series = (y == '>50K').astype(int)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=TEST_SIZE, \n",
    "                                                        shuffle=True, \n",
    "                                                        stratify=y, \n",
    "                                                        random_state=RANDOM_STATE\n",
    "                                                        )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = map(lambda x: x.reset_index(drop=True), [X_train, X_test, y_train, y_test])\n",
    "\n",
    "    if isinstance(params['train_size'], float):\n",
    "        assert params['train_size'] <= 1, f'{params['train_size']=} must be <= 1'\n",
    "        train_len = X_train.shape[0]\n",
    "        X_train = X_train.iloc[np.arange(0, int(train_len * params['train_size']))] # Можем, потому что shuffle=True\n",
    "        y_train = y_train.iloc[np.arange(0, int(train_len * params['train_size']))]\n",
    "\n",
    "    elif isinstance(params['train_size'], int):\n",
    "        assert X_train.shape[0] >= params['train_size'], f\"{params['train_size']=} must be <=  {X_train.shape[0]=}\"\n",
    "        X_train = X_train.iloc[np.arange(0, params['train_size'])] # Можем, потому что shuffle=True\n",
    "        y_train = y_train.iloc[np.arange(0, params['train_size'])]\n",
    "\n",
    "    cats_preprocessor = PREPROCESSORS[params['cats_encoder']['name']](**params['cats_encoder']['params'])\n",
    "\n",
    "    if params['num_scalers']['name'] is not None:\n",
    "        num_preprocessor = PREPROCESSORS[params['num_scalers']['name']](**params['num_scalers']['params'])\n",
    "    else:\n",
    "        num_preprocessor = Pipeline([(\"_\", \"passthrough\")])\n",
    "\n",
    "    X_train_cats = cats_preprocessor.fit_transform(X_train[cat_features])\n",
    "    X_test_cats = cats_preprocessor.transform(X_test[cat_features])\n",
    "\n",
    "    X_train_num = num_preprocessor.fit_transform(X_train[num_features])\n",
    "    X_test_num = num_preprocessor.transform(X_test[num_features])\n",
    "\n",
    "    X_train = np.hstack([X_train_cats, X_train_num])\n",
    "    X_test = np.hstack([X_test_cats, X_test_num])\n",
    "\n",
    "    logger.info(f'    Размер тренировочного датасета: {len(y_train)}')\n",
    "    logger.info(f'    Размер тестового датасета: {len(y_test)}')\n",
    "\n",
    "    logger.info('Начали сохранять датасеты')\n",
    "    os.makedirs(os.path.dirname(DATASET_PATH_PATTERN), exist_ok=True)\n",
    "    for split, split_name in zip((X_train, X_test, y_train, y_test),\n",
    "                                 ('X_train', 'X_test', 'y_train', 'y_test'),\n",
    "                                 ):\n",
    "        pd.DataFrame(split).to_csv(DATASET_PATH_PATTERN.format(split_name=split_name), index=False)\n",
    "    logger.info('Успешно сохранили датасеты!')\n",
    "\n",
    "    mlflow.log_params({\"dataset_name\": DATASET_NAME,\n",
    "                       \"n_features\": len(columns),\n",
    "                       \"features\": \",\".join(columns),\n",
    "                       \"train_size_rows\": int(len(y_train)),\n",
    "                       \"test_size_rows\": int(len(y_test)),\n",
    "                       \"cat_features\": \",\".join(cat_features),\n",
    "                       \"num_features\": \",\".join(num_features),\n",
    "                       \"cats_encoder_name\": params['cats_encoder']['name'],\n",
    "                       \"cats_encoder_params\": str(params['cats_encoder']['params']),\n",
    "                       \"num_scaler_name\": params['num_scalers']['name'],\n",
    "                       \"num_scaler_params\": str(params['num_scalers']['params']),\n",
    "                       \"random_state\": RANDOM_STATE})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92be9d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-17 08:32:02,078 : INFO : process_data : Начали скачивать данные\n",
      "2026-02-17 08:32:04,055 : INFO : process_data : Успешно скачали данные!\n",
      "2026-02-17 08:32:04,056 : INFO : process_data : Делаем предобработку данных\n",
      "2026-02-17 08:32:04,073 : INFO : process_data :     Используемые фичи: ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country']\n",
      "2026-02-17 08:32:04,213 : INFO : process_data :     Размер тренировочного датасета: 18233\n",
      "2026-02-17 08:32:04,214 : INFO : process_data :     Размер тестового датасета: 9769\n",
      "2026-02-17 08:32:04,215 : INFO : process_data : Начали сохранять датасеты\n",
      "2026-02-17 08:32:06,061 : INFO : process_data : Успешно сохранили датасеты!\n",
      "2026-02-17 08:32:06,121 : INFO : train : Начали считывать датасеты\n",
      "2026-02-17 08:32:06,409 : INFO : train : Успешно считали датасеты!\n",
      "2026-02-17 08:32:06,409 : INFO : train : Создаём модель\n",
      "2026-02-17 08:32:06,410 : INFO : train :     Параметры модели: {'model': 'LogisticRegression', 'model_args': {'penalty': 'l2', 'C': 0.9, 'solver': 'lbfgs', 'max_iter': 1000, 'random_state': 42}}\n",
      "2026-02-17 08:32:06,411 : INFO : train : Обучаем модель\n",
      "2026-02-17 08:32:07,021 : INFO : train : Сохраняем модель\n",
      "2026-02-17 08:32:07,026 : INFO : train : Успешно!\n",
      "2026-02-17 08:32:07,035 : INFO : evaluate : Начали считывать датасеты\n",
      "2026-02-17 08:32:07,374 : INFO : evaluate : Успешно считали датасеты!\n",
      "2026-02-17 08:32:07,375 : INFO : evaluate : Загружаем обученную модель\n",
      "2026-02-17 08:32:07,377 : INFO : evaluate : Скорим модель на тесте\n",
      "2026-02-17 08:32:20,353 : INFO : evaluate : Значения скоров: \n",
      "{'roc_auc_score': {'best_score': 0.8980535671866158, 'best_thresh': None}, 'average_precision_score': {'best_score': 0.7443424184871776, 'best_thresh': None}, 'recall_score': {'best_score': 1.0, 'best_thresh': np.float64(0.0)}, 'f1_score': {'best_score': 0.6853868194842407, 'best_thresh': np.float64(0.328)}}\n"
     ]
    }
   ],
   "source": [
    "from scripts import process_data, train, evaluate\n",
    "\n",
    "process_data()\n",
    "train()\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620c27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc_score': {'best_score': 0.5, 'best_tresh': None},\n",
       " 'average_precision_score': {'best_score': 0.6, 'best_tresh': None},\n",
       " 'accuracy_score': {'best_score': 0.6, 'best_tresh': np.float64(0.6)},\n",
       " 'precision_score': {'best_score': 0.6, 'best_tresh': np.float64(0.6)},\n",
       " 'recall_score': {'best_score': 1.0, 'best_tresh': np.float64(0.6)},\n",
       " 'f1_score': {'best_score': 0.75, 'best_tresh': np.float64(0.6)}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             roc_auc_score,\n",
    "                             average_precision_score)\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        category=UndefinedMetricWarning)\n",
    "\n",
    "\n",
    "def find_best_tresh(y_true: np.ndarray, \n",
    "                    y_probas: np.ndarray,\n",
    "                    scorer: Callable,\n",
    "                    ) -> tuple[float, float]:\n",
    "    '''-> (best_score, best_trash)''' \n",
    "    # Буду через сетку перебирать, хоть и не очень правильно\n",
    "    grid_n = 1_000\n",
    "    grid = np.arange(0, grid_n) / grid_n\n",
    "    bool_preds = y_probas[None,:] <= grid[:,None]\n",
    "    scores = [scorer(y_true, bool_preds[i]) for i in range(grid_n)]\n",
    "    best_tresh = np.argmax(scores)\n",
    "    \n",
    "    return scores[best_tresh], best_tresh / grid_n\n",
    "\n",
    "def get_scores(scorers_with_tresh: list[callable],\n",
    "               scorers_without_tresh: list[callable],\n",
    "               y_true: np.ndarray,\n",
    "               y_probas: np.ndarray,\n",
    "               ) -> dict[str, dict[float, float|None]]:\n",
    "    '''-> {scorer_name: {best_score: float,\n",
    "                         best_tresh, float|None}}\n",
    "    '''\n",
    "    scores = {scorer.__name__: {\"best_score\": scorer(y_true, y_probas),\n",
    "                                'best_tresh': None\n",
    "                                } \n",
    "              for scorer in scorers_without_tresh}\n",
    "    for scorer in scorers_with_tresh:\n",
    "        score, tresh = find_best_tresh(y_true, y_probas, scorer)\n",
    "        scores[scorer.__name__]  = {'best_score': score, 'best_tresh': tresh}\n",
    "    \n",
    "    return scores\n",
    "\n",
    "get_scores(SCORERS_WITH_THESH,\n",
    "           SCORERS_WITHOUT_THESH,\n",
    "           np.array([1, 1, 0, 1, 0]),\n",
    "           np.array([3/5, 3/5, 3/5, 3/5, 3/5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfe8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(501)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas = np.array([1/2 , 1/3, 0.99])\n",
    "tr = np.array([1 , 1, 0])\n",
    "r = probas[None,:] < (np.arange(0, 1_000) / 1_000)[:,None]\n",
    "np.argmax([accuracy_score(tr, r[i]) for i in range(1_000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7edc6813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 1.000e-04, 2.000e-04, ..., 9.997e-01, 9.998e-01,\n",
       "        9.999e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(0, 10_000) / 10_000)[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "# precision\n",
    "# recall\n",
    "# f1-score\n",
    "# ROC-AUC\n",
    "# PR-AUC\n",
    "#      6. артефакты (один из следующих типов):\n",
    "\n",
    "# classification report\n",
    "# confusion matrix\n",
    "# feature importances\n",
    "# csv с ошибками модели\n",
    "# PR-кривая"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
